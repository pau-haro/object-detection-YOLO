# 游 PROYECTO DE PRUEBA DE DETECCI칍N DE OBJETOS

Este proyecto tiene como objetivo **entrenar un modelo YOLO** (You Only Look Once) para la **detecci칩n de objetos en im치genes personalizadas**, creando un flujo completo desde el **etiquetado de datos** hasta el **entrenamiento final del modelo**.  

Se utiliza **Label Studio** para generar las anotaciones en formato YOLO y la librer칤a **Ultralytics** para gestionar el entrenamiento y validaci칩n del modelo, permitiendo su ejecuci칩n tanto en **PC como en dispositivos embebidos** (como Raspberry Pi).  

El prop칩sito final es disponer de un **modelo funcional y adaptable**, capaz de reconocer objetos definidos por el usuario en distintos entornos, con un proceso de entrenamiento **reproducible, automatizado y optimizado** dentro de un entorno virtual controlado mediante **Anaconda**. 

---

## ENTORNO VIRTUALIZADO DE EJECUCI칍N

Para poder hacer este proyecto de manera eficaz, se recomienda disponer de ***Anaconda***, un **gestor de entornos python** que nos permite disponer de las **versiones 칩ptimas** tanto de *python* como de las *librer칤as* para que el programa funcione sin fallos de incompatibilidad entre librer칤as.

Por ello, el **primer paso** es *tener Anaconda instalado* en nuestro dispositivo, en caso de no tenerlo, puedes descargarlo desde [**este enlace**](https://www.anaconda.com/download).

***Una vez instalado*** **creamos el entorno de python** desde el que trabajaremos mediante el siguiente comando (**en caso de no tenerlo ya creado**), el cual *se ha de ejecutar desde Anaconda prompt, o desde un powershell habilitado* para ejecuci칩n de comandos Anaconda: 

```bash

# Creamos el entorno indicando el nombre y la versi칩n de python deseada
conda create --name nombreEntorno python=3.12

```

Lo **siguiente** una vez creado **es activarlo**, que podemos hacer de la siguiente manera:

```bash

# Activar el entorno creado
conda activate nombreEntorno

```

Y una vez probado todo, **desactivar** el entorno:

```bash

# Desactivar el entorno creado
conda deactivate

```

---

## ETIQUETADO DE LAS IM츼GENES

Para el etiquetado de las im치genes utilizarmos una **herramienta** de python de **open source** que llamada ***label studio***, que se instalar칤a de la siguiente manera (como puede verse tambi칠n en la [p치gina oficial](https://labelstud.io/)):

```bash

# Instalamos los paquetes dentro del entorno python en de Anaconda
pip install -U label-studio

# Lo lanzamos a ejecuci칩n
label-studio

```

Una vez lanzado a ejecuci칩n, se nos abrir치 una ventana en el navegador como la que se ve a continuaci칩n.

![Ventana de inicio](src/image.png)

Como se ve, esto se lanza en un **servidor local**, por lo que lo ideal es registrarse con un correo inventado, ya que *no requiere de verificaci칩n al tratarse de un **daemon** al que 칰nicamente nos podemos conectar de manera local*.

Una vez registrado, el **siguiente paso es iniciar sesi칩n con el correo y contrase침a del registro**, y accederemos a la ventana de los proyectos, pero **si ya** anteriormente **has iniciado sesi칩n**, aunque hayas cerrado el programa, p*asas directamente a la ventana de proyectos*.

![Ventana de proyectos](src/image-1.png)

En esta ventana tendr칤amos que **crear el proyecto** y **a침adir las im치genes** que *deseamos etiquetar*, que, idealmente tendr칤amos que tener en una carpeta.

Para ello, le damos a **crear proyecto**, indicamos el nombre, **a침adimos las muestras**, y antes de guardar, **nos dirigimos al** apartado de `Labeling Setup`, y seleccionar la opci칩n de `object detection with bounding boxes`, como se ve en la siguiente imagen.

![Labeling Setup](src/image-2.png)

Una vez guardamos, el **siguiente paso** es **etiquetar los objetos** cen sus respectivas clases, para ello, s*eleccionamos la imagen*, y se abrir치 un men칰, *seleccionaremos la clase del objeto*, y *recuadraremos el objeto correspondiente* como se puede ver en la imagen.

![Objeto etiquetado en la imagen](src/image-3.png)

**Una vez etiquetadas**, *exportamos el proyecto desde el men칰 principal* del proyecto de las im치genes etiquetadas (**formato YOLO with images**), *nos lo descargar치 en formato .zip*, lo **recomendable** es *almacenarlo en la carpeta de im치genes* del proyecto

---

## ENTRENAMIENTO DEL MODELO

En este proyecto **se usa** [Ultralytics](https://docs.ultralytics.com/es/) **para entrenar los modelos** YOLO11, YOLOv8 o YOLOv5 en detecci칩n de objetos **con un dataset custom**. La ***finalidad*** de este proyecto es *tener un c칩digo funcional para entrenar nuestro propio modelo YOLO para correr desde nuestro propio PC, tel칠fono o incluso en una Raspberry Pi*.

El **primer paso** es ***renombrar*** el archivo comprimido *a data.zip* y ***a침adirlo*** a la *carpeta ra칤z* de nuestro proyecto, donde ***lo extraeremos***.

El **siguiente paso** es *ejecutar* el c칩digo `train_val_split.py` para *dividir el dataset en train y validation*, al cual le tenemos que ***pasar como aprgumentos*** la *ruta de la carpeta generada al descomprimir data.zip* y el *porcentaje de datos que queremos en la carpeta de training* (sobre 1, ej: `0.8` ), este 칰ltimo es opcional porque se pone a *0.8 como defaultValue*. Este c칩digo se encuentra en la carpeta `/scripts`.

```bash

# Divisi칩n del conjunto
python .\train_val_split.py --datapath="..\data" --train_pct=0.9    # Ubicados en la carpeta del script

```

Despu칠s de esto, es **necesario instalar la librer칤a de ultralytics** para poder entrenar nuestro modelo YOLO.

```bash

# Instalaci칩n de la librer칤a
pip install ultralytics

```

Una vez instalada la librer칤a de ultralytics, el siguiente paso es crear el archivo de configuraci칩n YAML de ultralytics, donde se ha de especificar la ubicaci칩n de los datos de entrenamiento y validaci칩n (carpetas train y validation)

Para crear el archivo de configuraci칩n podemos ejecutar el c칩digo **genetate_yaml.py** ubicado en la carpeta `/scripts`. Y una vez ejecutado, nos generar칤a autom치ticamente un archivo data.yml similar a este:

```yaml

path: /content/data
train: train/images
val: validation/images
nc: 1
names:
- Mug

```

Una vez disponemos del archivo YAML, podemos entrenar el modelo, pero antes de eso, es necesario instalar el resto de librer칤as necesarias:

```bash

# Instalaci칩n de las librer칤as necesarias
pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124

```



--> **POR TERMINAR** <--