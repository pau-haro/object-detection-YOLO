# üß† PROYECTO DE PRUEBA DE DETECCI√ìN DE OBJETOS

Este proyecto tiene como objetivo **entrenar un modelo YOLO** (You Only Look Once) para la **detecci√≥n de objetos en im√°genes personalizadas**, creando un flujo completo desde el **etiquetado de datos** hasta el **entrenamiento final del modelo**.  

Se utiliza **Label Studio** para generar las anotaciones en formato YOLO y la librer√≠a **Ultralytics** para gestionar el entrenamiento y validaci√≥n del modelo, permitiendo su ejecuci√≥n tanto en **PC como en dispositivos embebidos** (como Raspberry Pi).  

El prop√≥sito final es disponer una plantilla para un **modelo funcional y adaptable**, capaz de reconocer objetos definidos por el usuario en distintos entornos, con un proceso de entrenamiento **reproducible, automatizado y optimizado** dentro de un entorno virtual controlado mediante **Anaconda**. 

---

## ENTORNO VIRTUALIZADO DE EJECUCI√ìN

Para poder hacer este proyecto de manera eficaz, se recomienda disponer de ***Anaconda***, un **gestor de entornos python** que nos permite disponer de las **versiones √≥ptimas** tanto de *python* como de las *librer√≠as* para que el programa funcione sin fallos de incompatibilidad entre librer√≠as.

Por ello, el **primer paso** es *tener Anaconda instalado* en nuestro dispositivo, en caso de no tenerlo, puedes descargarlo desde [**este enlace**](https://www.anaconda.com/download).

***Una vez instalado*** **creamos el entorno de python** desde el que trabajaremos mediante el siguiente comando (**en caso de no tenerlo ya creado**), el cual *se ha de ejecutar desde Anaconda prompt, o desde un powershell habilitado* para ejecuci√≥n de comandos Anaconda: 

```bash

# Creamos el entorno indicando el nombre y la versi√≥n de python deseada
conda create --name nombreEntorno python=3.12

```

Lo **siguiente** una vez creado **es activarlo**, que podemos hacer de la siguiente manera:

```bash

# Activar el entorno creado
conda activate nombreEntorno

```

Y una vez probado todo, **desactivar** el entorno:

```bash

# Desactivar el entorno creado
conda deactivate

```

---

## ETIQUETADO DE LAS IM√ÅGENES

Para el etiquetado de las im√°genes utilizarmos una **herramienta** de python de **open source** que llamada ***label studio***, que se instalar√≠a de la siguiente manera (como puede verse tambi√©n en la [p√°gina oficial](https://labelstud.io/)):

```bash

# Instalamos los paquetes dentro del entorno python en de Anaconda
pip install -U label-studio

# Lo lanzamos a ejecuci√≥n
label-studio

```

Una vez lanzado a ejecuci√≥n, se nos abrir√° una ventana en el navegador como la que se ve a continuaci√≥n.

![Ventana de inicio](src/image.png)

Como se ve, esto se lanza en un **servidor local**, por lo que lo ideal es registrarse con un correo inventado, ya que *no requiere de verificaci√≥n al tratarse de un **daemon** al que √∫nicamente nos podemos conectar de manera local*.

Una vez registrado, el **siguiente paso es iniciar sesi√≥n con el correo y contrase√±a del registro**, y accederemos a la ventana de los proyectos, pero **si ya** anteriormente **has iniciado sesi√≥n**, aunque hayas cerrado el programa, p*asas directamente a la ventana de proyectos*.

![Ventana de proyectos](src/image-1.png)

En esta ventana tendr√≠amos que **crear el proyecto** y **a√±adir las im√°genes** que *deseamos etiquetar*, que, idealmente tendr√≠amos que tener en una carpeta.

Para ello, le damos a **crear proyecto**, indicamos el nombre, **a√±adimos las muestras**, y antes de guardar, **nos dirigimos al** apartado de `Labeling Setup`, y seleccionar la opci√≥n de `object detection with bounding boxes`, como se ve en la siguiente imagen.

![Labeling Setup](src/image-2.png)

Una vez guardamos, el **siguiente paso** es **etiquetar los objetos** cen sus respectivas clases, para ello, s*eleccionamos la imagen*, y se abrir√° un men√∫, *seleccionaremos la clase del objeto*, y *recuadraremos el objeto correspondiente* como se puede ver en la imagen.

![Objeto etiquetado en la imagen](src/image-3.png)

**Una vez etiquetadas**, *exportamos el proyecto desde el men√∫ principal* del proyecto de las im√°genes etiquetadas (**formato YOLO with images**), *nos lo descargar√° en formato .zip*, lo **recomendable** es *almacenarlo en la carpeta de im√°genes* del proyecto

---

## ENTRENAMIENTO DEL MODELO

En este proyecto **se usa** [Ultralytics](https://docs.ultralytics.com/es/) **para entrenar los modelos** YOLO11, YOLOv8 o YOLOv5 en detecci√≥n de objetos **con un dataset custom**. La ***finalidad*** de este proyecto es *tener un c√≥digo funcional para entrenar nuestro propio modelo YOLO para correr desde nuestro propio PC, tel√©fono o incluso en una Raspberry Pi*.

El **primer paso** es ***renombrar*** el archivo comprimido *a data.zip* y ***a√±adirlo*** a la *carpeta ra√≠z* de nuestro proyecto, donde ***lo extraeremos***.

Despu√©s de esto, es **necesario instalar la librer√≠a de ultralytics y pytorch** para poder entrenar y utilizar nuestro modelo YOLO.

```bash

# Instalaci√≥n de las librer√≠as necesarias para entrenar el modelo

# Ultralytics
pip install ultralytics

# torch, torchvision y torchaudio
pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124

# Probamos a ver si se ha instalado correctamente
python -c "import torch; print(torch.cuda.get_device_name(0))"

```

El **siguiente paso** es *ejecutar* el c√≥digo `train_val_split.py` para *dividir el dataset en train y validation*, al cual le tenemos que ***pasar como aprgumentos*** la *ruta de la carpeta generada al descomprimir data.zip* y el *porcentaje de datos que queremos en la carpeta de training* (sobre 1, ej: `0.8` ), este √∫ltimo es opcional porque se pone a *0.8 como defaultValue*. Este c√≥digo se encuentra en la carpeta `/scripts`.

```bash

# Divisi√≥n del conjunto
python .\train_val_split.py --datapath="..\data" --train_pct=.9    # Ubicados en la carpeta del script

```

Una vez **instalada la librer√≠a de ultralytics**, el ***siguiente paso*** es *crear el archivo de configuraci√≥n YAML* de ultralytics, donde se ha de *especificar la ubicaci√≥n de los datos* de entrenamiento y validaci√≥n (carpetas train y validation)

Para crear el archivo de configuraci√≥n podemos ejecutar el c√≥digo **generate_yaml.py** ubicado en la carpeta `/scripts`. 

```bash

# Ubicados en la carpeta scripts (desde terminal)
python generate_yaml.py

```

Y una vez ejecutado, nos generar√≠a autom√°ticamente un archivo data.yml similar a este:

```yaml

path: /content/data
train: train/images
val: validation/images
nc: 1
names:
- Mug

```

Una vez tenemos el archivo de configuraci√≥n, el siguiente paso es ***entrenar el modelo***, para eso tenemos que *ejecutar el comando* (ubicados en la carpeta ra√≠z del proyecto):

```bash
yolo detect train data=data/config/data.yaml model=yolo11x.pt epochs=150 imgsz=640 patience=20 project="../models" name=""

```

Estos son los **principales par√°metros** utilizados para entrenar el modelo YOLO:

| Par√°metro | Descripci√≥n |
|------------|-------------|
| `--data` | **Archivo de configuraci√≥n YAML** que define las rutas de las im√°genes de entrenamiento, validaci√≥n y las clases. <br>Ejemplo: `data/custom_data.yaml`. |
| `--epochs` | **N√∫mero de √©pocas de entrenamiento.** <br>Indica cu√°ntas veces el modelo ver√° todo el conjunto de datos durante el entrenamiento. <br>Valores comunes: `50`, `100`, `200`, etc. |
| `--imgsz` | **Tama√±o (resoluci√≥n) de las im√°genes** utilizadas durante el entrenamiento. <br>Ejemplo: `640` o `416`. <br>Un valor mayor puede mejorar la precisi√≥n, pero aumenta el tiempo de entrenamiento. |
| `--model` | **Modelo base YOLO** que se desea utilizar o ruta a un modelo personalizado. <br>Ejemplo: `yolov8n.pt`, `yolov8s.pt` o `runs/detect/train/weights/best.pt`. |


El ***modelo lo escoger√≠amos en base a las necesidades del entrenamiento*** y los resultados deseados, y podr√≠amos escoger entre los siguientes modelos:

![Modelos Yolo para usar](src/image-4.png)

Una vez finalizado el entrenamiento, el siguiente paso es correr el modelo, esto lo har√≠amos ejecutando el c√≥digo **yolo_detection.py** ubicado en la carpeta `scripts/`.

```bash

# En --model hay que indicar la ubicaci√≥n EN NUESTRO ORDENADOR en la que se enecuentra el modelo
# Esta ubicaci√≥n la conoceremos cuando se acabe de entrenar el modelo, nos saldr√° indicado en el terminal. O en nuestro caso, ubicado en la carpeta models

# Lo ejecuta en una webcam USB
python yolo_detection.py --model=runs/detect/train/weights/best.pt --source=usb0  

# Lo ejecuta sobre test_vid.mp4 a 1280x720 
python yolo_detection.py --model=yolo11s.pt --source=test_vid.mp4 resolution=1280x720  

```

***El script acepta varios argumentos*** que *permiten personalizar la ejecuci√≥n* del modelo YOLO seg√∫n las necesidades del usuario:

| Argumento | Descripci√≥n |
|------------|-------------|
| `--model` | **Ruta al modelo YOLO entrenado** (`.pt`). <br>Ejemplo: `runs/detect/train/weights/best.pt`. <br>Este archivo contiene los pesos del modelo que se utilizar√°n para la detecci√≥n. |
| `--source` | **Fuente de entrada** para la detecci√≥n. Puede ser:<br>‚Ä¢ Una imagen (`test.jpg`)<br>‚Ä¢ Una carpeta con im√°genes (`test_dir`)<br>‚Ä¢ Un v√≠deo (`testvid.mp4`)<br>‚Ä¢ Una c√°mara USB (`usb0`)<br>‚Ä¢ Una c√°mara Raspberry Pi (`picamera0`). |
| `--min_conf` | **Umbral m√≠nimo de confianza** para mostrar detecciones. <br>Valores entre `0.0` y `1.0` (por defecto `0.5`). <br>Cuanto m√°s alto, menos detecciones se mostrar√°n (m√°s estrictas). |
| `--resolution` | **Resoluci√≥n de salida del v√≠deo o ventana de inferencia**, en formato `AnchoxAlto` (por ejemplo `640x480`). <br>Si no se especifica, se usa la resoluci√≥n de la fuente original. |
| `--record` | **Guarda los resultados del v√≠deo o c√°mara** en un archivo (por defecto `demo1.avi`). <br>‚ö†Ô∏è Solo se puede usar si se especifica tambi√©n `--resolution`. |

---

## üë®‚Äçüíª **Autor**

Proyecto desarrollado por **Pau Haro Ac√≠n** para **TSE Technology Solutions**

üìß **Contacto:** [pau.haro@tsetechnology.com](mailto:pau.haro@tsetechnology.com)

---

## ü§ù **Cr√©ditos y uso**

Si este trabajo te resulta √∫til en tus investigaciones, publicaciones o desarrollos,  
por favor **menciona este repositorio** o incluye una referencia al autor. üôå

```text
¬© 2025 [TSE Technology Solutions]. Todos los derechos reconocidos.
Distribuido bajo MIT License.
